{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ce3523b-f487-475d-b323-111d3b7c492f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72d58fdc-245e-4265-a037-8d5b5762b112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aff94e11-fa12-4f9a-9de8-4a18c1da6910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Create Widgets\n",
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.dropdown(\"is_drop_managed_tables\", \"False\", [\"True\", \"False\"], \"Drop Managed Tables?\")\n",
    "dbutils.widgets.text(\"external_table_pii_location\", \"abfss://\", \"External Storage (PII)\")\n",
    "dbutils.widgets.text(\"external_table_nonpii_location\", \"abfss://\", \"External Storage (Non-PII)\")\n",
    "dbutils.widgets.text(\"yaml_configuration\", \"\", \"YAML table list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f6dae1-d8cf-4fd1-9150-e271edbb112c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Configure logging\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "level = logging.INFO\n",
    "logging.basicConfig(stream=sys.stderr,level=level,format=\"%(asctime)s [%(name)s][%(levelname)s] %(message)s\")\n",
    "logging.getLogger(\"databricks.sdk\").setLevel(level)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82fd2b46-27e3-4e7b-8e68-debea14bb39d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Set Widgets\n",
    "external_table_pii_location = dbutils.widgets.get(\"external_table_pii_location\")\n",
    "\n",
    "external_table_nonpii_location = dbutils.widgets.get(\"external_table_nonpii_location\")\n",
    "\n",
    "is_drop_managed_tables = dbutils.widgets.get(\"is_drop_managed_tables\")\n",
    "\n",
    "yaml_configuration = dbutils.widgets.get(\"yaml_configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f67ce3da-39af-432a-9ebe-56df4729a3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import lit\n",
    "pd.set_option(\"display.max_rows\", None) \n",
    "\n",
    "class HMSTableMigration:\n",
    "    '''\n",
    "        Contains a list of utility methods to migrate managed tables to external tables within hive_metastore.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, spark, yaml_configuration):\n",
    "        self.spark = spark\n",
    "        self.yaml_configuration = yaml_configuration\n",
    "        self.managed_table_list = pd.DataFrame()\n",
    "        self.managed_to_external_failed_tables = []\n",
    "        self.converted_tables_list = []\n",
    "\n",
    "        \n",
    "    def _load_migration_conf(self, yaml_configuration,root=\"resources\",leaf=\"tables\"):\n",
    "        logger.info(f\"yaml file {yaml_configuration}\")\n",
    "        # Load all migration tables yaml\n",
    "        with open(yaml_configuration, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            if(len(yaml_configuration) > 0):\n",
    "                if bool(config):\n",
    "                    try:\n",
    "                        pdf = pd.DataFrame.from_dict(config[root][leaf],orient=\"index\").sort_values(by='t_no')\n",
    "                        display(pdf)\n",
    "                        if leaf == \"tables\":\n",
    "                            self.managed_table_list = pdf   \n",
    "                    except Exception as exception:\n",
    "                        logger.info(\"Cant find any excluded tables in the list given or not in correct format\")\n",
    "                        raise\n",
    "        \n",
    "        return self.managed_table_list,config\n",
    "\n",
    "    def check_point(self,df):\n",
    "        df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\"checkpoint.csv\")\n",
    "\n",
    "    def check_table_exist(self,table_name):\n",
    "        return spark.catalog.tableExists(f\"{table_name}\")    \n",
    "    \n",
    "    def check_table_is_managed(self,database_name,managed_table,catalog=\"hive_metastore\"):\n",
    "        \"\"\"Table is External return False\n",
    "           Table is Managed return True\n",
    "        \"\"\"\n",
    "        if self.check_table_exist(f\"`{catalog}`.`{database_name}`.`{managed_table}`\"):\n",
    "            val = \"\"\n",
    "            df_managed = spark.sql(f\"\"\"DESCRIBE TABLE EXTENDED\n",
    "                                    {catalog}.`{database_name}`.`{managed_table}`\"\"\").where(\"col_name = 'Type'\").collect()\n",
    "            for row in df_managed:\n",
    "                val = row[\"data_type\"]\n",
    "\n",
    "            if str(val).lower() ==  \"managed\":\n",
    "                return True\n",
    "            else:\n",
    "                False\n",
    "        return False\n",
    "    \n",
    "    def assert_managed_vs_external(self, database_name, managed_table):\n",
    "        # Register Managed Table &. get count\n",
    "        managed_table = spark.table(f\"{database_name}.{managed_table}_old\")\n",
    "        managed_table_count = managed_table.count()\n",
    "\n",
    "        # Register External Table & get count\n",
    "        external_table = spark.table(f\"{database_name}.{managed_table}\")\n",
    "        external_table_count = external_table.count()\n",
    "\n",
    "        # Assert both are matching\n",
    "        assert managed_table_count == external_table_count\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    #    Migrate MANAGED Tables\n",
    "    # -------------------------------\n",
    "\n",
    "    def upgrade_hm_managed_to_hm_external(self, database_name, managed_table, location,table_number):\n",
    "        '''\n",
    "            Upgrades the Managed Table to the External Table within Hive Metastore\n",
    "\n",
    "            Parameters:\n",
    "                database_name:          Name of the hive metastore database where table resides\n",
    "                new_database_name:      Name of the unity database where table will be recreated\n",
    "                managed_table:          The full table name to be converted\n",
    "                external_table:         The new full table after conversion\n",
    "                location:               The destination where the table will be recreated based on pii flag.\n",
    "        '''\n",
    "        \n",
    "\n",
    "        # Use the provided HM Database\n",
    "        spark.sql(f\"USE {database_name}\")\n",
    "        tablesize_in_mbs = 0\n",
    "        logger.info(f\"Transfer started : {database_name}.{managed_table}\")\n",
    "        try:\n",
    "            #Timer Started\n",
    "            start_time = datetime.datetime.now()\n",
    "            if self.check_table_exist(f\"`hive_metastore`.{database_name}.{managed_table}_ext\"):\n",
    "                logger.info(f\"External table {managed_table}_ext already exist\")\n",
    "                logger.info(f\"Transfer skipped : {database_name}.{managed_table}\")\n",
    "\n",
    "            else:   \n",
    "                if self.check_table_is_managed(database_name,f\"{managed_table}\"):\n",
    "                    print(f\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS {managed_table}_ext\n",
    "                        DEEP CLONE {managed_table} \n",
    "                        LOCATION '{location}'\n",
    "                    \"\"\")\n",
    "                    spark.sql(f\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS {managed_table}_ext\n",
    "                        DEEP CLONE {managed_table} \n",
    "                        LOCATION '{location}'\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    for i in spark.sql(f\"describe detail `hive_metastore`.{database_name}.{managed_table}\").select(\"sizeInBytes\").collect():\n",
    "                        tablesize_in_mbs = (i[\"sizeInBytes\"]/(1000*1000))\n",
    "                    try:\n",
    "                        if not self.check_table_exist(f\"`hive_metastore`.{database_name}.{managed_table}_old\") and self.check_table_is_managed(database_name,f\"{managed_table}\"):\n",
    "                            spark.sql(f\"ALTER TABLE {managed_table} RENAME TO {managed_table}_old\")\n",
    "                            logger.info(f\"Renaming completed! Managed table is {managed_table}_old\")\n",
    "                        else:\n",
    "                            logger.info(f\"Table :{database_name}.{managed_table}_old already exists\")    \n",
    "                    except Exception as exception:\n",
    "                        full_table_name_source = f\"{database_name}.{managed_table}\"\n",
    "                        logger.info(f\"Failed to rename the managed table :{database_name}.{managed_table} due to: {str(exception)}\")\n",
    "                        self.managed_to_external_failed_tables.append((full_table_name_source,table_number,str(exception)))\n",
    "                        pass\n",
    "                    try:\n",
    "                        if  self.check_table_exist(f\"`hive_metastore`.{database_name}.{managed_table}_ext\") and not self.check_table_is_managed(database_name,f\"{managed_table}_ext\"):\n",
    "                            spark.sql(f\"ALTER TABLE {managed_table}_ext RENAME TO {managed_table}\")\n",
    "                            logger.info(f\"Renaming completed! External table is {managed_table}\")\n",
    "                        else:\n",
    "                            logger.info(f\"Table :{database_name}.{managed_table}_ext does not exists\") \n",
    "                    except Exception as exception:\n",
    "                        logger.info(f\"Failed to rename the external table :{database_name}.{managed_table}_ext due to: {str(exception)}\")\n",
    "                        self.managed_to_external_failed_tables.append((full_table_name_source,table_number,str(exception)))\n",
    "                        pass\n",
    "                    end_time = datetime.datetime.now()\n",
    "                    duration = end_time - start_time\n",
    "                    duration_in_s = duration.seconds\n",
    "                    m, s = divmod(duration_in_s, 60)\n",
    "                    m = int(m)\n",
    "                    s = int(s) \n",
    "                    rate = 0.0\n",
    "                    if duration_in_s != 0:\n",
    "                        rate = tablesize_in_mbs/duration_in_s             \n",
    "                    self.converted_tables_list.append((database_name,managed_table,f'{m:02d}:{s:02d}',tablesize_in_mbs,rate))        \n",
    "\n",
    "        except Exception as exception:\n",
    "            logger.info(f\"Failed to clone managed table {database_name}.{managed_table} due to: {str(exception)}\")\n",
    "            full_table_name_source = f\"{database_name}.{managed_table}\"\n",
    "            self.managed_to_external_failed_tables.append((full_table_name_source,table_number,str(exception)))\n",
    "            pass\n",
    "        \n",
    "\n",
    "        # Assert the count\n",
    "        # assert_managed_vs_external(managed_table)\n",
    "        if(is_drop_managed_tables == \"True\" and self.check_table_is_managed(database_name,f\"{managed_table}_old\")):\n",
    "            spark.sql(f\"DROP TABLE {managed_table}_old\")\n",
    "            logger.info(f\"Dropping completed! Managed table removed {managed_table}_old\")\n",
    "\n",
    "    def execute(self):\n",
    "        # Load the configuration file\n",
    "        mtl,config = self._load_migration_conf(yaml_configuration)\n",
    "\n",
    "        # Loop through each table and run the conversion\n",
    "        # for table in config['resources']['tables']:\n",
    "        for index, row in mtl.iterrows():    \n",
    "            # managed_table_obj = config['resources']['tables'][table]\n",
    "            database_name = row['database']\n",
    "            managed_table = row['name']\n",
    "            external_table = row['target_table']\n",
    "            is_pii = row['is_pii']\n",
    "            new_database_name = row['target_schema']\n",
    "            table_number = row['t_no']\n",
    "\n",
    "            location = (lambda is_pii, external_table_nonpii_location, external_table_pii_location, managed_table: \n",
    "                f\"{external_table_pii_location if is_pii else external_table_nonpii_location}/{new_database_name}/{external_table}\"\n",
    "                )(is_pii, external_table_nonpii_location, external_table_pii_location, managed_table)\n",
    "            \n",
    "            self.upgrade_hm_managed_to_hm_external(database_name, managed_table, location,table_number)\n",
    "\n",
    "    #Getting the status of the migration        \n",
    "    def printMigrationStats(self):\n",
    "        '''\n",
    "            Prints migration statistics.\n",
    "        '''\n",
    "        logger.info(\"CONVERSION RESULTS\")\n",
    "        logger.info(\"-------------------------------------------------------------------\")\n",
    "        logger.info(f\"TABLES CANNOT CONVERT TO EXTERNAL: {len(self.managed_to_external_failed_tables)}\")\n",
    "        if (len(self.managed_to_external_failed_tables) > 0):\n",
    "            all_not_converted = self.managed_to_external_failed_tables\n",
    "            non_dbfs_root = [x for x in all_not_converted]\n",
    "            if (len(non_dbfs_root) > 0):\n",
    "                display(non_dbfs_root)\n",
    "\n",
    "        logger.info(f\"TABLES CONVERTED TO EXTERNAL: {len(self.converted_tables_list)}\") \n",
    "        if (len(self.converted_tables_list ) > 0):\n",
    "            all_not_converted = self.converted_tables_list\n",
    "            non_dbfs_root = [x for x in all_not_converted]\n",
    "            columns = [\"Target Schema\",\"Target Table Name\",\"Time elapsed to convert\",\"Table size (Mb)\",\"Transfer speed(MbPS)\"]\n",
    "            if (len(non_dbfs_root) > 0):\n",
    "                sparkdf = spark.createDataFrame(non_dbfs_root,columns)\n",
    "                display(sparkdf)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcaabb96-d088-4c6d-b2a4-e3c19dbdf713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "internal_migration = HMSTableMigration(spark, yaml_configuration)\n",
    "internal_migration.execute()\n",
    "internal_migration.printMigrationStats()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "HMUpgradeToExternal",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
