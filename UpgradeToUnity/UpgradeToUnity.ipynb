{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f4653c5-4bb5-45c3-82f1-ba097fb86328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Catalog migration script\n",
    "\n",
    "### Description\n",
    "This script is an utility to migrate a catalog from hive_metastore to Unity Catalog. It loops through all/specified schemas in the hive_metastore and migrates the following:\n",
    "- external tables\n",
    "- views\n",
    "\n",
    "This script can run in DRY RUN mode in which case it will only print the feasibility of the migration table per table basis.\n",
    "\n",
    "### How to use\n",
    "#### Parameters:\n",
    "- Schemas to migrate (`schemas-to-migrate`): default is 'all'; specify schemas separated by comma if 'all' is not desired\n",
    "- Migration scope (`migration-scope`): default is 'all'; possible values 'external-tables', 'views'\n",
    "- UC Catalog Destination (`uc-catalog-destination`): catalog destination in Unity catalog\n",
    "- UC Schema Owner (`uc-schema-owner`): owner for the newly created UC schemas, tables & views; this needs to be an account level group / user\n",
    "- DRY RUN (`dry_run mode`): True/False; if True it won't perform the migration, it will print to the INFO log the result of the attempted migration\n",
    "- Table list for migration (`selective-table-upgrade-yaml`): YAML file which contains the table list to migrate\n",
    "- Storage accounts:\n",
    "    - Storage accounts for non PII external locations(`external-locations-non-pii`): storage account list (separated by comma) of all external location for non PII data\n",
    "    - Storage accounts for PII external locations(`external-locations-pii`): storage account list (separated by comma) of all external location for PII data\n",
    "- Azure Service Principal authentication details:\n",
    "    - DB Secret Scope for SP credentials (`auth-secret-scope`): Databricks secrets scope where the Service Principals secrets are saved\n",
    "    - SP Non PII Secret Value DB Secrets (`auth-client-secret-non-pii`): Databricks secrets key where the Service Principal secret for PII data is saved\n",
    "    - SP PII Secret Value DB Secrets (`auth-client-secret-pii`): Databricks secrets key where the Service Principal secret for PII data is saved\n",
    "    - SP Non PII ID (`auth-client-id-non-pii`): Service Principal ID used to access non PII data \n",
    "    - SP PII ID (`auth-client-id-pii`): Service Principal ID used to access PII data\n",
    "    - SP Endpoint(`auth-client-endpoint`): Service Principal Endpoint\n",
    "\n",
    "#### How to run:\n",
    "- setup the parameters in the defined widgets and run all the cells\n",
    "- note this needs to run from a UC compatible single user cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00cfc228-2254-4db1-acfc-a1eada9881ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26ffc4dc-1fb4-4cd5-89a0-cb46f3226127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83e3d9fc-6fbe-4296-ae05-7b5fa17a03fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "196c927c-7755-4ad7-87ce-4e7ffdf05ece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"selective-table-upgrade-yaml\", \"\", \"Selective Table Upgrade Config Path\")\n",
    "dbutils.widgets.text(\"uc-catalog-destination\", \"\", \"\")\n",
    "dbutils.widgets.text(\"uc-schema-owner\", \"\", \"\")\n",
    "dbutils.widgets.dropdown(\"dry-run\",choices=[\"True\",\"False\"],defaultValue=\"True\")\n",
    "dbutils.widgets.text(\"schemas-to-migrate\", \"\", \"\")\n",
    "dbutils.widgets.dropdown(\"migration-scope\",choices=[\"all\",\"external-tables\",\"views\"],defaultValue=\"external-tables\")\n",
    "dbutils.widgets.text(\"exclude-schemas\", \"\", \"\")\n",
    "dbutils.widgets.text(\"external-locations-non-pii\", \"\", \"\")\n",
    "dbutils.widgets.text(\"external-locations-pii\", \"\", \"\")\n",
    "dbutils.widgets.text(\"auth-secret-scope\", \"\", \"\")\n",
    "dbutils.widgets.text(\"auth-client-id-non-pii\", \"\", \"\")\n",
    "dbutils.widgets.text(\"auth-client-id-pii\", \"\", \"\")\n",
    "dbutils.widgets.text(\"auth-client-secret-non-pii\", \"\", \"\")\n",
    "dbutils.widgets.text(\"auth-client-secret-pii\", \"\", \"\")\n",
    "dbutils.widgets.text(\"auth-client-endpoint\", \"\", \"\")\n",
    "dbutils.widgets.text(\"table-exclusion-yaml\", \"\", \"\")\n",
    "dbutils.widgets.text(\"selective-view-upgrade-yaml\", \"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11cee979-dd8c-4891-bbac-2ed3f6d84fa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_destination = dbutils.widgets.get(\"uc-catalog-destination\")\n",
    "owner_destination = dbutils.widgets.get(\"uc-schema-owner\")\n",
    "dry_run = True if dbutils.widgets.get(\"dry-run\") == \"True\" else False\n",
    "schemas_to_migrate = dbutils.widgets.get(\"schemas-to-migrate\")\n",
    "migration_scope = dbutils.widgets.get(\"migration-scope\")\n",
    "exclude_schemas = dbutils.widgets.get(\"exclude-schemas\")\n",
    "external_locations_non_pii = dbutils.widgets.get(\"external-locations-non-pii\")\n",
    "external_locations_pii = dbutils.widgets.get(\"external-locations-pii\")\n",
    "auth_secret_scope = dbutils.widgets.get(\"auth-secret-scope\")\n",
    "auth_client_id_non_pii = dbutils.widgets.get(\"auth-client-id-non-pii\")\n",
    "auth_client_id_pii = dbutils.widgets.get(\"auth-client-id-pii\")\n",
    "auth_client_secret_non_pii = dbutils.widgets.get(\"auth-client-secret-non-pii\")\n",
    "auth_client_secret_pii = dbutils.widgets.get(\"auth-client-secret-pii\")\n",
    "auth_client_endpoint = dbutils.widgets.get(\"auth-client-endpoint\")\n",
    "table_exclusion_yaml = dbutils.widgets.get(\"table-exclusion-yaml\")\n",
    "selected_table_list_yaml =  dbutils.widgets.get(\"selective-table-upgrade-yaml\")\n",
    "if table_exclusion_yaml and selected_table_list_yaml:\n",
    "    raise ValueError(\"Input either selective upgrade YAML file or exclusion table YAML file\")\n",
    "yaml_configuration = \"\"\n",
    "if table_exclusion_yaml:\n",
    "    yaml_configuration = table_exclusion_yaml\n",
    "else:\n",
    "    yaml_configuration = selected_table_list_yaml\n",
    "\n",
    "change_view_path_yaml = dbutils.widgets.get(\"selective-view-upgrade-yaml\")\n",
    "\n",
    "# validate input parameters\n",
    "if not catalog_destination:\n",
    "    raise ValueError(\"UC Catalog Destination is required for the migration\")\n",
    "if not owner_destination:\n",
    "    raise ValueError(\"UC Owner Destination is required for the migration\")\n",
    "if not schemas_to_migrate:\n",
    "    raise ValueError(\"Specify which schemas to migrate, separated by comma. Leave 'all' and it will migrate all schemas.\")\n",
    "if not external_locations_non_pii:\n",
    "    raise ValueError(\"External Locations for Non PII data not specified.\")\n",
    "else:\n",
    "    external_locations_non_pii_list = external_locations_non_pii.replace(' ','').split(',')\n",
    "if not external_locations_pii:\n",
    "    raise ValueError(\"External Locations for PII data not specified.\")\n",
    "else:\n",
    "    external_locations_pii_list = external_locations_pii.replace(' ','').split(',')\n",
    "if not (auth_secret_scope and auth_client_id_non_pii and auth_client_id_pii and auth_client_secret_non_pii and auth_client_secret_pii and auth_client_endpoint):\n",
    "    raise ValueError(\"Azure SP credentials are required for the migration.\")\n",
    "\n",
    "## Authenticate with Service Principals for Non PII External Location\n",
    "for external_locations_non_pii_item in external_locations_non_pii_list:\n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{external_locations_non_pii_item}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_locations_non_pii_item}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_locations_non_pii_item}.dfs.core.windows.net\", auth_client_id_non_pii) # f19cea40-ebb7-4350-8be4-72056fa25585\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_locations_non_pii_item}.dfs.core.windows.net\", dbutils.secrets.get(scope=auth_secret_scope, key=auth_client_secret_non_pii)) # {{secrets/Databricks-BIS-PP-EUW/Databricks-Spn-BusinessIntelligenceSystems-PreProd}}\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_locations_non_pii_item}.dfs.core.windows.net\", auth_client_endpoint) #  https://login.microsoftonline.com/f009f285-5242-433a-9365-daa1edf145c3/oauth2/token\n",
    "\n",
    "## Authenticate with Service Principals for PII External Location\n",
    "for external_locations_pii_item in external_locations_pii_list:\n",
    "    spark.conf.set(f\"fs.azure.account.auth.type.{external_locations_pii_item}.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth.provider.type.{external_locations_pii_item}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.id.{external_locations_pii_item}.dfs.core.windows.net\", auth_client_id_pii)\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{external_locations_pii_item}.dfs.core.windows.net\", dbutils.secrets.get(scope=auth_secret_scope, key=auth_client_secret_pii))\n",
    "    spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{external_locations_pii_item}.dfs.core.windows.net\", auth_client_endpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dad7c1c7-99e6-466e-b285-f2589774819e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7ac6290-7e33-4e13-a4d2-e259a92eca11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Configure logging\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# level = logging.DEBUG if dbutils.widgets.get(\"debug\") == \"True\" else logging.INFO\n",
    "level = logging.INFO\n",
    "logging.basicConfig(stream=sys.stderr,level=level,format=\"%(asctime)s [%(name)s][%(levelname)s] %(message)s\")\n",
    "logging.getLogger(\"databricks.sdk\").setLevel(level)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06fdced5-fbdf-437e-bfef-2b87f40e00fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Migration code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bea1dbe4-813b-4f73-a3d8-351c667f5a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import yaml\n",
    "import pyspark.sql.utils\n",
    "from pyspark.sql.functions import concat_ws, col, lit\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "## Configure logging\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "level = logging.INFO\n",
    "logging.basicConfig(stream=sys.stderr,level=level,format=\"%(asctime)s [%(name)s][%(levelname)s] %(message)s\")\n",
    "logging.getLogger(\"databricks.sdk\").setLevel(level) \n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class CatalogMigration:\n",
    "    '''\n",
    "        Contains a list of utility methods to migrate external tables and views form hive_metastore to Unity Catalog.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.statements = []\n",
    "        self.migrated_tables = []\n",
    "        self.not_eligible_for_migration = []\n",
    "        self.failed_migration_tables = []\n",
    "        self.migrated_views = []\n",
    "        self.failed_migration_views = []\n",
    "        columns_empty = StructType([])\n",
    "        self.exclusion_table_list = spark.createDataFrame([],columns_empty)\n",
    "        self.migration_view_list = spark.createDataFrame([],columns_empty)\n",
    "\n",
    "    def _load_migration_conf(self, yaml_configuration,root,leaf):\n",
    "        '''\n",
    "        Load the YAML file to databricks dataframe\n",
    "        Parameters:\n",
    "            yaml_configuration: YAML file path\n",
    "            root: root of the YAML file\n",
    "            leaf: leaf of the yaml file\n",
    "\n",
    "        Returns:\n",
    "            return exclution table list databricks dataframe , viee list databricks dataframe , yaml raw output   \n",
    "        '''\n",
    "        logger.info(f\"yaml file {yaml_configuration}\")\n",
    "        # Load all migration tables yaml\n",
    "        with open(yaml_configuration, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            if(len(yaml_configuration) > 0):\n",
    "                if bool(config):\n",
    "                    try:\n",
    "                        pdf = pd.DataFrame.from_dict(config[root][leaf],orient=\"index\")\n",
    "                        if leaf == \"tables\":\n",
    "                            df = spark.createDataFrame(pdf).dropDuplicates()\n",
    "                            self.exclusion_table_list = df\n",
    "                        if leaf == \"views\":\n",
    "                            df_v = spark.createDataFrame(pdf).dropDuplicates()\n",
    "                            self.migration_view_list = df_v\n",
    "                            \n",
    "                    except Exception as exception:\n",
    "                        logger.info(\"Cant find any excluded tables in the list given or not in correct format\")\n",
    "                        raise\n",
    "            \n",
    "        return self.exclusion_table_list,self.migration_view_list,config\n",
    "\n",
    "    # -------------------------------\n",
    "    #    Migrate VIEWS\n",
    "    # -------------------------------\n",
    "    def _view_definition_replace_table(self, view_definition:str):\n",
    "        '''\n",
    "            replace the view definition with migrated tables and return the new view definition\n",
    "            Parameters:\n",
    "                view_definition:  The existing view definition\n",
    "            \n",
    "            Returns:\n",
    "               String new view definition\n",
    "        '''\n",
    "        if yaml_configuration:\n",
    "            tablelist,_,_=self._load_migration_conf(yaml_configuration,\"resources\",\"tables\")\n",
    "            full_table_name_source = list(tablelist.withColumn(\"full_table_name_source\", concat_ws(\".\",col(\"database\"), col(\"name\")))\n",
    "                                .select(\"full_table_name_source\")\n",
    "                                .toPandas()['full_table_name_source'])\n",
    "            full_table_name_destination = list(tablelist.withColumn(\"full_table_name_destination\", concat_ws(\".\",col(\"target_schema\"), col(\"target_table\")))\n",
    "                                .select(\"full_table_name_destination\")\n",
    "                                .toPandas()['full_table_name_destination'])\n",
    "            for old,new in zip(full_table_name_source,full_table_name_destination):\n",
    "                view_definition=view_definition.replace(old,new)      \n",
    "        else:\n",
    "            logger.error(\"Cannot find external table list\")\n",
    "            raise\n",
    "        return view_definition\n",
    "\n",
    "    def _migrate_view(self, full_view_name_source, catalog_destination, view_owner_to, dry_run,is_selective=False,destination_view=\"\"):\n",
    "        '''\n",
    "            Migrates a view from hive metastore to the catalog destination and sets the specified owner\n",
    "\n",
    "            Parameters:\n",
    "                full_view_name_source:  The full view name to be migrated\n",
    "                catalog_destination:    The catalog destination where the view will be recreated.\n",
    "                view_owner_to:          The owner of the newly created view.\n",
    "                dry_run:                When running in dry run mode, the view statement is only printed to the log and the view will not be created. \n",
    "                                        Note this does not validate if there are any errors in the view DDL statement.\n",
    "            \n",
    "            Returns:\n",
    "                True:                   If the view was successfully migrated or it was run in dry run mode\n",
    "                False:                  If the view migration failed\n",
    "        '''\n",
    "\n",
    "        use_hive_catalog_statement = \"USE CATALOG `hive_metastore`\"\n",
    "        self.spark.sql(use_hive_catalog_statement)\n",
    "        try:\n",
    "            # Get the DDL of the view\n",
    "            view_definition_statement = f\"DESCRIBE EXTENDED {full_view_name_source}\"\n",
    "            self.statements.append(view_definition_statement)\n",
    "            view_definition_ddl = self.spark.sql(view_definition_statement).where(\"col_name = 'View Text'\").collect()\n",
    "            # Only one row expected here\n",
    "            for row in view_definition_ddl:\n",
    "                view_definition = row['data_type']\n",
    "                # Replace hive_metastore if present with the catalog destination\n",
    "                view_definition = re.sub(rf\"(`?hive_metastore`?)\", f\"`{catalog_destination}`\", view_definition)\n",
    "                if not destination_view:\n",
    "                    full_view_name_destination = full_view_name_source.replace(\"hive_metastore\", catalog_destination)\n",
    "                else:    \n",
    "                    full_view_name_destination = destination_view\n",
    "                    view_definition=self._view_definition_replace_table(view_definition)\n",
    "                \n",
    "\n",
    "\n",
    "                use_uc_catalog_statement = f\"USE CATALOG `{catalog_destination}`\" # Most of the views won't have hive_metastore in the DDL, so we need to switch to the catalog destination\n",
    "                view_statement = f\"CREATE OR REPLACE VIEW {full_view_name_destination} AS {view_definition}\"\n",
    "                alter_owner_statement = f\"ALTER VIEW {full_view_name_destination} OWNER TO `{view_owner_to}`\" # Set new owner\n",
    "                \n",
    "                \n",
    "                if (dry_run):\n",
    "                    logger.info(use_uc_catalog_statement)\n",
    "                    logger.info(view_statement)\n",
    "                    logger.info(alter_owner_statement)\n",
    "                    logger.info(f\"{use_hive_catalog_statement}\\n\")\n",
    "                else:\n",
    "\n",
    "                    self.statements.append(use_uc_catalog_statement)\n",
    "                    self.spark.sql(use_uc_catalog_statement)\n",
    "                        \n",
    "                    self.statements.append(view_statement)\n",
    "                    self.spark.sql(view_statement)\n",
    "                        \n",
    "                    self.statements.append(alter_owner_statement)\n",
    "                    self.spark.sql(alter_owner_statement)\n",
    "                        \n",
    "                    self.migrated_views.append(full_view_name_source)\n",
    "                    logger.info(f\"Successfully migrated view: {full_view_name_source}\")\n",
    "\n",
    "                    # Revert back to using hive_metastore\n",
    "                    self.statements.append(use_hive_catalog_statement)\n",
    "                    spark.sql(use_hive_catalog_statement)\n",
    "\n",
    "                return True\n",
    "        except Exception as exception:\n",
    "            logger.error(f\"Failed migrating view: {full_view_name_source} due to: {str(exception)}\")\n",
    "            self.failed_migration_views.append(full_view_name_source)\n",
    "            # Revert back to using hive_metastore\n",
    "            if not dry_run:\n",
    "                self.statements.append(use_hive_catalog_statement)\n",
    "                self.spark.sql(use_hive_catalog_statement)\n",
    "            return False\n",
    "\n",
    "       \n",
    "    \n",
    "    def _migrate_views(self, views, catalog_destination, owner_destination, dry_run,destination_views):\n",
    "        '''\n",
    "            Migrates all views from the from hive_metatore to the specified UC catalog\n",
    "\n",
    "             Parameters:\n",
    "                views:                  The list of full view name to be migrated\n",
    "                catalog_destination:    The catalog destination where the view will be recreated.\n",
    "                view_owner_to:          The owner of the newly created view.\n",
    "                dry_run:                When running in dry run mode, the view statement is only printed to the log and the view will not be created. \n",
    "                                        Note this does not validate if there are any errors in the view DDL statement.\n",
    "        '''\n",
    "        if change_view_path_yaml:\n",
    "            for view,destination_view in zip(views,destination_views):\n",
    "                self._migrate_view(view,catalog_destination, owner_destination, dry_run,is_selective=True,destination_view=destination_view)\n",
    "        else:\n",
    "            for view in views:\n",
    "                self._migrate_view(view, catalog_destination, owner_destination, dry_run)\n",
    "\n",
    "    def _get_views_per_schema(self, schema_name):\n",
    "        '''\n",
    "            Get all views from the schema\n",
    "\n",
    "            Parameters:\n",
    "                schema_name:            Schema to get the views for\n",
    "\n",
    "            Returns:\n",
    "                list of complete view names (including catalog, schema, view name) existent in the hive_metastore specified schema\n",
    "        '''\n",
    "        show_views_statement = f\"SHOW VIEWS FROM hive_metastore.{schema_name}\"\n",
    "        self.statements.append(show_views_statement)\n",
    "        try:\n",
    "            return list(self.spark.sql(show_views_statement)\n",
    "                        .withColumn(\"full_view_name\", concat_ws(\".\", lit('hive_metastore'), col(\"namespace\"), col(\"viewName\")))\n",
    "                        .select(\"full_view_name\")\n",
    "                        .toPandas()['full_view_name'])\n",
    "        except Exception as exception:\n",
    "            logger.error(f\"Cannot retrieve views for schema: {schema_name} due to: {str(exception)}\")\n",
    "            return list()\n",
    "    \n",
    "    def _get_views(self, schemas,catalog_destination):\n",
    "        '''\n",
    "            Returns list of views from all schemas\n",
    "\n",
    "            Parameters:\n",
    "                schemas:                List of schemas\n",
    "\n",
    "            Returns:\n",
    "                list of complete view names for all schemas\n",
    "        '''\n",
    "        views = []\n",
    "        destination_views = []\n",
    "        try:\n",
    "            if change_view_path_yaml:\n",
    "                _,data,_ = self._load_migration_conf(change_view_path_yaml,\"resources\",\"views\")\n",
    "                views.extend(list(data.withColumn(\"full_view_name\", concat_ws(\".\", lit('hive_metastore'), col(\"source_schema\"), col(\"source_view\")))\n",
    "                            .select(\"full_view_name\")\n",
    "                            .toPandas()['full_view_name']))\n",
    "                destination_views.extend(list(data.withColumn(\"full_detination_view_name\", concat_ws(\".\", lit(f'{catalog_destination}'), col(\"target_schema\"), col(\"target_view\")))\n",
    "                            .select(\"full_detination_view_name\")\n",
    "                            .toPandas()['full_detination_view_name']))\n",
    "                logger.info(f\"Following views will be migrated {views} to {destination_views}\")\n",
    "            else:\n",
    "                for schema_name in schemas:\n",
    "                    use_hive_catalog_statement = \"USE CATALOG `hive_metastore`\"\n",
    "                    self.spark.sql(use_hive_catalog_statement)\n",
    "                    views.extend(self._get_views_per_schema(schema_name))\n",
    "        except Exception as exception:\n",
    "            logger.error(f\"Cannot retrieve views due to: {str(exception)}\")\n",
    "            return list()            \n",
    "        return views,destination_views\n",
    "    \n",
    "    # -------------------------------\n",
    "    #    Check exclude list\n",
    "    # -------------------------------\n",
    "    def _is_excluding(self, source_schema, source_table_name):\n",
    "        '''\n",
    "            Return status of a table is in exclusion list\n",
    "\n",
    "            Parameters:\n",
    "                source_schema:        Schema to exclude\n",
    "                source_table_name:    Table to exclude\n",
    "            Returns:\n",
    "                True:                 If table is in the list\n",
    "                False                 If table is not in the list or List is empty\n",
    "            \n",
    "        '''\n",
    "        df = self.exclusion_table_list\n",
    "        logger.info(f\"Following tables will get excluded {df}\")       \n",
    "        return not (df.filter( (df['schema']  == source_schema) & (df['table']  == source_table_name) ).isEmpty())\n",
    "\n",
    "    # -------------------------------\n",
    "    #    Migrate TABLES\n",
    "    # -------------------------------\n",
    "\n",
    "    def _migrate_table(self, full_table_name_source, full_table_name_destination, schema_owner_to, dry_run,is_selective=False):\n",
    "        '''\n",
    "            Migrates an external table using SYNC and sets the specified owner\n",
    "\n",
    "            Parameters:\n",
    "                full_table_name_source: The full table name to be migrated\n",
    "                catalog_destination:    The catalog destination where the table will be synced.\n",
    "                schema_owner_to:        The owner of the newly created table.\n",
    "                dry_run:                When running in dry run mode, this method is not executed. SYNC in Dry RUN mode is ran at schema level prior to this method.\n",
    "\n",
    "            Returns:\n",
    "                True:                   If the migration was successful or the method was running in dry run mode\n",
    "                False:                  If the migration failed\n",
    "        '''\n",
    "        if (not dry_run):\n",
    "            try:\n",
    "                sync_table_statement = f\"SYNC TABLE {full_table_name_destination} FROM {full_table_name_source} SET OWNER `{schema_owner_to}`\"\n",
    "                self.statements.append(sync_table_statement)\n",
    "                result = self.spark.sql(sync_table_statement).collect()\n",
    "                # only one row expected \n",
    "                for row in result:\n",
    "                    match row['status_code']:\n",
    "                        case 'SUCCESS':\n",
    "                            logger.info(f\"Successfully migrated table: {full_table_name_source} to {full_table_name_destination}\")\n",
    "                            self.migrated_tables.append(full_table_name_source)\n",
    "                            return True\n",
    "                        case _:\n",
    "                            status = row['status_code']\n",
    "                            description = row['description']\n",
    "                            logger.error(f\"Failed migrating table: {full_table_name_source} to {full_table_name_destination} due to: {status}: {description}\")\n",
    "                            self.failed_migration_tables.append(f\"{full_table_name_source}: error {status}, {description}\")\n",
    "                            return False\n",
    "            except Exception as exception:\n",
    "                logger.error(f\"Failed migrating table: {full_table_name_source} due to: {str(exception)}\")\n",
    "                self.failed_migration_tables.append(full_table_name_source)\n",
    "                return False\n",
    "            \n",
    "        elif(dry_run and is_selective):\n",
    "            try:\n",
    "                logger.info(f\"Migrating Table: {full_table_name_source}\")\n",
    "                sync_table_dry_run_statement = f\"SYNC TABLE {full_table_name_destination} FROM {full_table_name_source} DRY RUN\"\n",
    "                self.statements.append(sync_table_dry_run_statement)\n",
    "                upgrade_table_dry_run = self.spark.sql(sync_table_dry_run_statement).collect()\n",
    "\n",
    "                if (dry_run and upgrade_table_dry_run):\n",
    "                    display(upgrade_table_dry_run)\n",
    "                \n",
    "                # Get all tables in the schema\n",
    "                for row in upgrade_table_dry_run:\n",
    "                    full_table_name_source = f\"`hive_metastore`.`{row['source_schema']}`.`{row['source_name']}`\"\n",
    "                    full_table_name_destination = f\"`{row['target_catalog']}`.`{row['target_schema']}`.`{row['target_name']}`\"\n",
    "                    status_code = row['status_code']\n",
    "                    match status_code:\n",
    "                        case 'DRY_RUN_SUCCESS':\n",
    "                            logger.info(f\"Dry run Successful on table: {full_table_name_source} to {full_table_name_destination}\")\n",
    "                            self.migrated_tables.append(full_table_name_source)\n",
    "                        case 'VIEWS_NOT_SUPPORTED':\n",
    "                            logger.debug(\"Views are migrated using method `_migrate_view`.\")\n",
    "                        case _:\n",
    "                            reason_cannot_migrate = row['description']\n",
    "                            self.not_eligible_for_migration.append((full_table_name_source, status_code, reason_cannot_migrate))        \n",
    "            except Exception as exception:\n",
    "                logger.error(f\"Failed migrating table in dry_run mode {dry_run}: {full_table_name_source} due to: {str(exception)}\")\n",
    "                self.failed_migration_tables.append(full_table_name_source)\n",
    "                return False   \n",
    "        return True\n",
    "\n",
    "    def _migrate_schema(self, schema_to_upgrade, catalog_destination, schema_owner_to, dry_run):\n",
    "        '''\n",
    "            Migrates a schema; note only external tables are supported using this method.\n",
    "\n",
    "            Parameters:\n",
    "                catalog_destination:    The catalog destination where the schema will be synced.\n",
    "                schema_owner_to:        The owner of the newly created schema.\n",
    "                dry_run:                When running in dry run mode, this migration is not done and the feasibility status is printed.\n",
    "        '''\n",
    "        try:\n",
    "            logger.info(f\"Migrating schema: {schema_to_upgrade}\")\n",
    "            sync_schema_dry_run_statement = f\"SYNC SCHEMA `{catalog_destination}`.`{schema_to_upgrade}` FROM `hive_metastore`.`{schema_to_upgrade}` DRY RUN\"\n",
    "            self.statements.append(sync_schema_dry_run_statement)\n",
    "            upgrade_schema_dry_run = self.spark.sql(sync_schema_dry_run_statement).collect()\n",
    "\n",
    "            if (dry_run and upgrade_schema_dry_run):\n",
    "                display(upgrade_schema_dry_run)\n",
    "            \n",
    "            # Get all tables in the schema\n",
    "            for row in upgrade_schema_dry_run:\n",
    "                full_table_name_source = f\"`hive_metastore`.`{row['source_schema']}`.`{row['source_name']}`\"\n",
    "                full_table_name_destination = f\"`{row['target_catalog']}`.`{row['target_schema']}`.`{row['target_name']}`\"\n",
    "                status_code = row['status_code']\n",
    "                if not(self._is_excluding(source_schema=row['source_schema'],source_table_name=row['source_name'])):\n",
    "                    match status_code:\n",
    "                        case 'DRY_RUN_SUCCESS':\n",
    "                            self._migrate_table(full_table_name_source, full_table_name_destination, schema_owner_to, dry_run)\n",
    "                        case 'VIEWS_NOT_SUPPORTED':\n",
    "                            logger.debug(\"Views are migrated using method `_migrate_view`.\")\n",
    "                        case _:\n",
    "                            reason_cannot_migrate = row['description']\n",
    "                            self.not_eligible_for_migration.append((full_table_name_source, status_code, reason_cannot_migrate))\n",
    "                else:\n",
    "                    logger.info(f\"Excluding table {full_table_name_source}\")           \n",
    "        except Exception as exception:\n",
    "            logger.error(f\"Failed migrating schema in dry_run mode {dry_run}: {catalog_destination}.{schema_to_upgrade} due to: {str(exception)}\")\n",
    "    \n",
    "    def _migrate_schema_selective(self, catalog_destination, schema_owner_to, dry_run, yaml_configuration):\n",
    "        '''\n",
    "            Migrates a schema; note only external tables are supported using this method and it used the migration.yml for selective updates\n",
    "\n",
    "            Parameters:\n",
    "                catalog_destination:    The catalog destination where the schema will be synced.\n",
    "                schema_owner_to:        The owner of the newly created schema.\n",
    "                dry_run:                When running in dry run mode, this migration is not done and the feasibility status is printed.\n",
    "                yaml_configuration:     The yaml configuration for the selective sync for the tables with customizations.\n",
    "        '''\n",
    "        try:\n",
    "            logger.info(f\"Migrating schemas & tables selectively\")\n",
    "            # Load the configuration file\n",
    "        \n",
    "            _,_,config = self._load_migration_conf(yaml_configuration,\"resources\",\"tables\")\n",
    "\n",
    "            # Loop through each table and run the SYNC upgrades\n",
    "            for table in config['resources']['tables']:\n",
    "                managed_table_obj = config['resources']['tables'][table]\n",
    "                database_name = managed_table_obj['database']\n",
    "                managed_table = managed_table_obj['name']\n",
    "                target_schema = managed_table_obj['target_schema']\n",
    "                target_table = managed_table_obj['target_table']\n",
    "                is_pii = managed_table_obj['is_pii']\n",
    "                full_table_name_source = f\"`hive_metastore`.`{database_name}`.`{managed_table}`\"\n",
    "                full_table_name_destination = f\"`{catalog_destination}`.`{target_schema}`.`{managed_table}`\"\n",
    "                self._migrate_table(full_table_name_source, full_table_name_destination, schema_owner_to, dry_run,is_selective=True)\n",
    "                if managed_table != target_table and dry_run == False:\n",
    "                    spark.sql(f\"ALTER TABLE {full_table_name_destination} RENAME TO `{catalog_destination}`.`{target_schema}`.`{target_table}`\")\n",
    "                    logger.info(f\"Sucessfully renamed table {full_table_name_destination} to `{catalog_destination}`.`{target_schema}`.`{target_table}`\")\n",
    "\n",
    "\n",
    "        except Exception as exception:\n",
    "            logger.error(f\"Failed migrating schemas selectively in dry_run mode {dry_run}: {catalog_destination} due to: {str(exception)}\")\n",
    "\n",
    "    def _migrate_schemas(self, schemas_to_upgrade, catalog_destination, schema_owner_to, dry_run, yaml_configuration):\n",
    "        '''\n",
    "            Migrates all schemas specifies from hive_metastore to UC specified schema\n",
    "\n",
    "            Parameters:\n",
    "                catalog_destination:    The catalog destination where the schema will be synced.\n",
    "                schemas_to_upgrade:     List of schemas to upgrade\n",
    "                schema_owner_to:        The owner of the newly created schema.\n",
    "                dry_run:                When running in dry run mode, this migration is not done and the feasibility status is printed.\n",
    "        '''\n",
    "        if(len(yaml_configuration) > 0):\n",
    "            self._migrate_schema_selective(catalog_destination, schema_owner_to, dry_run, yaml_configuration)\n",
    "        else:\n",
    "            for schema_name in schemas_to_upgrade:\n",
    "                self._migrate_schema(schema_name, catalog_destination, schema_owner_to, dry_run)\n",
    "\n",
    "    def _get_schemas(self, schemas_to_migrate, exclude_schemas):\n",
    "        '''\n",
    "            Gets all schemas\n",
    "\n",
    "            Parameters:\n",
    "                schemas_to_migrate:     Input parameters from the wigdets: 'all', list of schemas specified by comma, individual schema names\n",
    "            \n",
    "            Returns:\n",
    "                schemas from hive_metastore based on the input parameters\n",
    "        '''\n",
    "        if (schemas_to_migrate == \"all\"):\n",
    "            # Get all schemas in the hive_metastore\n",
    "            schemas = list(self.spark.sql(\"SHOW schemas IN hive_metastore\").toPandas()['databaseName'])\n",
    "        else:\n",
    "            # Get all schemas separated by comma, trim white spaces\n",
    "            schemas = schemas_to_migrate.replace(' ','').split(',')\n",
    "        if (exclude_schemas != ''):\n",
    "            exclude_schemas_list = exclude_schemas.replace(' ','').split(',')\n",
    "            result = list (set(schemas) - set(exclude_schemas_list))\n",
    "        else:\n",
    "            result = schemas\n",
    "        return result\n",
    "    \n",
    "    def migrate_catalog(self, migration_scope, schemas_to_migrate, catalog_destination, schema_owner_to, exclude_schemas, dry_run=True, yaml_configuration = \"\"):\n",
    "        '''\n",
    "            Entry point for migrating the catalog. Migrates a catalog, only including external tables and views.\n",
    "\n",
    "            Parameters:\n",
    "                migration_scope:        What will be migrated: external-tables, views, all (all includes external tables & views)\n",
    "                schemas_to_migrate:     Which schemas will be migrated: specify list of comma separated schemas or 'all' (includes all schemas from hive_metastore) \n",
    "                catalog_destination:    The catalog destination where the catalog will be synced. Note this needs to exist before running this script.\n",
    "                schema_owner_to:        The owner of the newly created schema.\n",
    "                dry_run:                When running in dry run mode, this migration is not done and the feasibility status of migrating tables is printed. Note views are not included in the dry run.\n",
    "        '''\n",
    "        EmptyStatus,_,_ = self._load_migration_conf(yaml_configuration,\"resources\",\"tables\")\n",
    "        logger.info(f\"Following tables will be ignored {EmptyStatus.show(truncate=False)}\")\n",
    "            \n",
    "\n",
    "        schemas = self._get_schemas(schemas_to_migrate, exclude_schemas) \n",
    "\n",
    "        match migration_scope:\n",
    "            case 'external-tables':\n",
    "                self._migrate_schemas(schemas, catalog_destination, schema_owner_to, dry_run, yaml_configuration)\n",
    "            case 'views':\n",
    "                views,destination_views = self._get_views(schemas,catalog_destination)\n",
    "                self._migrate_views(views, catalog_destination, schema_owner_to, dry_run,destination_views)\n",
    "            case 'all':\n",
    "                views,destination_views = self._get_views(schemas,catalog_destination)\n",
    "                self._migrate_schemas(schemas, catalog_destination, schema_owner_to, dry_run, yaml_configuration)\n",
    "                self._migrate_views(views, catalog_destination, schema_owner_to, dry_run,destination_views)\n",
    "            case _:\n",
    "                raise ValueError(f\"Value '{migration_scope}' not recognized for migration scope. Please choose from: external-tables, views, all.\")\n",
    "\n",
    "    def printMigrationStats(self):\n",
    "        '''\n",
    "            Prints migration statistics.\n",
    "        '''\n",
    "        logger.info(\"MIGRATION RESULTS\")\n",
    "        logger.info(\"--------------------\")\n",
    "        logger.info(f\"NOT ELIGIBLE FOR MIGRATION: {len(self.not_eligible_for_migration)}\")\n",
    "        if (len(self.not_eligible_for_migration) > 0):\n",
    "            all_not_eligigle = self.not_eligible_for_migration\n",
    "\n",
    "            dbfs_root_non_default = [x for x in all_not_eligigle if x[1] == 'DBFS_ROOT_LOCATION' and \"default\" not in x[0]]\n",
    "            logger.info(f\"MANAGED TABLES not in 'default' schema: {len(dbfs_root_non_default)}\")\n",
    "            if (len(dbfs_root_non_default) > 0):\n",
    "                display(dbfs_root_non_default)\n",
    "\n",
    "            non_dbfs_root = [x for x in all_not_eligigle if x[1] != 'DBFS_ROOT_LOCATION']\n",
    "            logger.info(f\"NON MANAGED TABLES: {len(non_dbfs_root)}\")\n",
    "            if (len(non_dbfs_root) > 0):\n",
    "                display(non_dbfs_root)\n",
    "        logger.info(f\"SUCCESSFULLY MIGRATED TABLES: {len(self.migrated_tables)}\")\n",
    "        logger.info(f\"FAILED MIGRATION TABLES: {len(self.failed_migration_tables)}\")\n",
    "        [logger.info(item) for item in self.failed_migration_tables]\n",
    "        logger.info(f\"SUCCESSFULLY MIGRATED VIEWS: {len(self.migrated_views)}\")\n",
    "        logger.info(f\"FAILED MIGRATION VIEWS: {len(self.failed_migration_views)}\")\n",
    "        [logger.info(item) for item in self.failed_migration_views]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0288f7c7-66d8-40d4-81f0-d2078f18e396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73ca44b9-d5a0-4133-88c2-fa4d4afafa24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "migration = CatalogMigration(spark)\n",
    "migration.migrate_catalog(migration_scope, schemas_to_migrate, catalog_destination, owner_destination, exclude_schemas, dry_run, yaml_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4031ab9-0a26-43b3-b6eb-5f805a33bdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "migration.printMigrationStats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39ab7c4e-0322-4305-86f9-205d93509517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CatalogMigrationScript",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
