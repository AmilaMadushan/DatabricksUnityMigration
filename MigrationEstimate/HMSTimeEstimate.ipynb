{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c891813-4e0a-48b9-b932-271f4df258ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install PyYAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c066d2-e655-4e17-97be-e2ca0e36b763",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b4285b-5c93-4a37-b09d-cdda0ca58011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"yaml_configuration\", \"\", \"YAML table list\")\n",
    "dbutils.widgets.text(\"Transfer_Speed\", \"\", \"Transfer speed of the table MbPS\")\n",
    "dbutils.widgets.dropdown(\"log_level\", \"INFO\", [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"], \"Log Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa62177-b670-4319-948d-775e8e053a63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "yaml_configuration = dbutils.widgets.get(\"yaml_configuration\")\n",
    "Transfer_Speed = dbutils.widgets.get(\"Transfer_Speed\")\n",
    "log_level = dbutils.widgets.get(\"log_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05e2821b-5fa9-4f42-b6d1-5849eae62fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Configure logging\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "level = logging.INFO\n",
    "logging.basicConfig(stream=sys.stderr,level=level,format=\"%(asctime)s [%(name)s][%(levelname)s] %(message)s\")\n",
    "logging.getLogger(\"databricks.sdk\").setLevel(level)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.WARNING)\n",
    "logger = logging.getLogger()\n",
    "logging.getLogger().setLevel(getattr(logging, log_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "442bdc7a-e06e-44fc-a0a3-3cd2d56b3d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "from decimal import Decimal\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import lit\n",
    "pd.set_option(\"display.max_rows\", None) \n",
    "\n",
    "class HMSTimeEstimate:\n",
    "    '''\n",
    "        Contains a list of utility methods to migrate managed tables to external tables within hive_metastore.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, spark, yaml_configuration):\n",
    "      self.spark = spark\n",
    "      self.yaml_configuration = yaml_configuration\n",
    "      self.managed_table_list = pd.DataFrame()\n",
    "      self.managed_to_external_failed_tables = []\n",
    "      self.converted_tables_list = []\n",
    "\n",
    "        \n",
    "    def _load_migration_conf(self, yaml_configuration,root=\"resources\",leaf=\"tables\"):\n",
    "        \"\"\"\n",
    "        Load the migration configuration from a YAML file.\n",
    "\n",
    "        Args:\n",
    "            yaml_configuration (str): Path to the YAML configuration file.\n",
    "            root (str): Root key in the YAML file to look for the configuration. Default is \"resources\".\n",
    "            leaf (str): Leaf key in the YAML file to look for the configuration. Default is \"tables\".\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the managed table list DataFrame and the entire configuration dictionary.\n",
    "        \"\"\"\n",
    "        logger.info(f\"yaml file {yaml_configuration}\")\n",
    "        # Load all migration tables yaml\n",
    "        with open(yaml_configuration, 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            if(len(yaml_configuration) > 0):\n",
    "                if bool(config):\n",
    "                    try:\n",
    "                        pdf = pd.DataFrame.from_dict(config[root][leaf],orient=\"index\").sort_values(by='t_no')\n",
    "                        display(pdf)\n",
    "                        if leaf == \"tables\":\n",
    "                            self.managed_table_list = pdf   \n",
    "                    except Exception as exception:\n",
    "                        logger.info(\"Cant find any excluded tables in the list given or not in correct format\")\n",
    "                        raise\n",
    "\n",
    "        return self.managed_table_list,config\n",
    "\n",
    "    def check_point(self,df):\n",
    "        \"\"\"\n",
    "        Save the DataFrame to a CSV file in the current working directory.\n",
    "\n",
    "        Args:\n",
    "            df (DataFrame): The DataFrame to save.\n",
    "        \"\"\"\n",
    "        df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\"checkpoint.csv\")\n",
    "\n",
    "    def check_table_exist(self,table_name):\n",
    "        \"\"\"\n",
    "        Check if a table exists in the Spark catalog.\n",
    "\n",
    "        Args:\n",
    "        table_name (str): The name of the table to check.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if the table exists, False otherwise.\n",
    "        \"\"\"\n",
    "        return spark.catalog.tableExists(f\"{table_name}\")    \n",
    "    \n",
    "    def check_table_is_managed(self,database_name,managed_table,catalog=\"hive_metastore\"):\n",
    "        \"\"\"\n",
    "        Check if a table is managed or external.\n",
    "\n",
    "        Args:\n",
    "            database_name (str): The name of the database.\n",
    "            managed_table (str): The name of the table to check.\n",
    "            catalog (str): The catalog to use. Default is \"hive_metastore\".\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the table is managed, False if it is external.\n",
    "        \"\"\"\n",
    "        if self.check_table_exist(f\"`{catalog}`.`{database_name}`.`{managed_table}`\"):\n",
    "            val = \"\"\n",
    "            df_managed = spark.sql(f\"\"\"DESCRIBE TABLE EXTENDED\n",
    "                                    {catalog}.`{database_name}`.`{managed_table}`\"\"\").where(\"col_name = 'Type'\").collect()\n",
    "            for row in df_managed:\n",
    "                val = row[\"data_type\"]\n",
    "\n",
    "            if str(val).lower() ==  \"managed\":\n",
    "                return True\n",
    "            else:\n",
    "                False\n",
    "        return False\n",
    "\n",
    "    # -------------------------------\n",
    "    #    Migrate MANAGED Tables\n",
    "    # -------------------------------\n",
    "\n",
    "    def hm_managed_to_hm_external_estimate(self, database_name, managed_table,table_number):\n",
    "        \"\"\"\n",
    "            Upgrades the Managed Table to the External Table within Hive Metastore\n",
    "\n",
    "            Parameters:\n",
    "                database_name:          Name of the hive metastore database where table resides\n",
    "                new_database_name:      Name of the unity database where table will be recreated\n",
    "                managed_table:          The full table name to be converted\n",
    "                external_table:         The new full table after conversion\n",
    "                location:               The destination where the table will be recreated based on pii flag.\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # Use the provided HM Database\n",
    "        spark.sql(f\"USE {database_name}\")\n",
    "        tablesize_in_mbs = 0\n",
    "        rate = 0.0\n",
    "        logger.info(f\"Estimate Started for Table : {database_name}.{managed_table}\")\n",
    "\n",
    "        if not self.check_table_exist(f\"`hive_metastore`.`{database_name}`.`{managed_table}`\"):\n",
    "                logger.info(f\"Estimation skipped because table not exist : {database_name}.{managed_table}\")\n",
    "\n",
    "        else:\n",
    "          try:\n",
    "\n",
    "            if self.check_table_is_managed(database_name,f\"{managed_table}\"):\n",
    "      \n",
    "              for i in spark.sql(f\"describe detail `hive_metastore`.{database_name}.{managed_table}\").select(\"sizeInBytes\").collect():\n",
    "                  tablesize_in_mbs = (i[\"sizeInBytes\"]/(1000*1000))\n",
    "\n",
    "              duration_in_s = tablesize_in_mbs * float(Transfer_Speed)\n",
    "              m,s = divmod(duration_in_s, 60)\n",
    "              m = int(m)\n",
    "              s = int(s)\n",
    "              self.converted_tables_list.append((database_name,managed_table,tablesize_in_mbs,f'{m:02d}:{s:02d}',duration_in_s)) \n",
    "              logger.info(f\"Estimate Ended for Table : {database_name}.{managed_table}\")       \n",
    "\n",
    "          except Exception as exception:\n",
    "              logger.info(f\"Failed to clone managed table {database_name}.{managed_table} due to: {str(exception)}\")\n",
    "              full_table_name_source = f\"{database_name}.{managed_table}\"\n",
    "              self.managed_to_external_failed_tables.append((full_table_name_source,table_number,str(exception)))\n",
    "              pass\n",
    "        \n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"\n",
    "        Execute the migration process by loading the configuration and converting tables.\n",
    "        \"\"\"\n",
    "        # Load the configuration file\n",
    "        mtl,config = self._load_migration_conf(yaml_configuration)\n",
    "\n",
    "        # Loop through each table and run the conversion\n",
    "        # for table in config['resources']['tables']:\n",
    "        for index, row in mtl.iterrows():    \n",
    "            # managed_table_obj = config['resources']['tables'][table]\n",
    "            database_name = row['database']\n",
    "            managed_table = row['name']\n",
    "            external_table = row['target_table']\n",
    "            is_pii = row['is_pii']\n",
    "            new_database_name = row['target_schema']\n",
    "            table_number = row['t_no']\n",
    "            \n",
    "            self.hm_managed_to_hm_external_estimate(database_name, managed_table,table_number)\n",
    "\n",
    "    #Getting the status of the migration        \n",
    "    def printMigrationStats(self):\n",
    "        \"\"\"\n",
    "        Print migration statistics including the number of evaluated tables and estimated times.\n",
    "        \"\"\"\n",
    "        logger.info(f\"EVALATED TABLES : {len(self.converted_tables_list)}\") \n",
    "        if (len(self.converted_tables_list ) > 0):\n",
    "            all_not_converted = self.converted_tables_list\n",
    "            non_dbfs_root = [x for x in all_not_converted]\n",
    "            columns = [\"Target Schema\",\"Target Table Name\",\"Table size (Mb)\",\"Estiamted Elapsed time to convert\",\"Estiamted Elapsed time to convert in seconds\"]\n",
    "            if (len(non_dbfs_root) > 0):\n",
    "                sparkdf = spark.createDataFrame(non_dbfs_root,columns)\n",
    "                aggrigatedval = sparkdf.select('Estiamted Elapsed time to convert in seconds').agg({'Estiamted Elapsed time to convert in seconds': 'sum'})\n",
    "                aggrigatedval= aggrigatedval.withColumn('Estiamted Elapsed time to convert in Minutes', aggrigatedval[\"sum(Estiamted Elapsed time to convert in seconds)\"]/60).withColumn('Estiamted Elapsed time to convert in Hours', aggrigatedval[\"sum(Estiamted Elapsed time to convert in seconds)\"]/(60*60))\n",
    "                display(sparkdf)\n",
    "                display(aggrigatedval)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2326e1b4-a3df-4e51-8d17-36f75556044f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "internal_migration = HMSTimeEstimate(spark, yaml_configuration)\n",
    "internal_migration.execute()\n",
    "internal_migration.printMigrationStats()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HMSTimeEstimate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
